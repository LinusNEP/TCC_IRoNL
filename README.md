<p align="center">

  <h2 align="center">Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models</h2>
  <p align="center">
      Team/Contributors:  <a href="https://cps.unileoben.ac.at/m-sc-linus/">Nwankwo Linus</a> | <a href="https://cps.unileoben.ac.at/prof-elmar/">Elmar Rueckert</a>
  </p>
  
<p align="center">
  <h3 align="center"> | <a href="https://human-llm-interaction.github.io/workshop/hri24/papers/hllmi24_paper_5.pdf">Paper</a> | <a href="https://arxiv.org/abs/2403.12273">arXiv</a> | <a href="https://creativecommons.org/licenses/by/4.0/">License</a> | </h3>
  <div align="center"></div>
</p>

## Multimodal TCC-IRoNL
The Multimodal TCC-IRoNL extends TCC-IRoNL to enable humans to interact naturally with autonomous agents through both spoken and textual conversations. Refer to the [paper here](https://human-llm-interaction.github.io/workshop/hri24/papers/hllmi24_paper_5.pdf) for more details.

## Citation
If you use this work in your research, please cite it using the following BibTeX entry:
```bibtex
@misc{nwankwo2024multimodal,
   title={Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models}, 
   author={Linus Nwankwo and Elmar Rueckert},
   year={2024},
   eprint={2403.12273},
   archivePrefix={arXiv},
   primaryClass={cs.RO}
}
```

## Multimodal TCC-IRoNL installation

**To be updated!**

## Run Multimodal TCC-IRoNL Example Demos

**To be updated!**


